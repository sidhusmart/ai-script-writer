{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script-write your soap opera - Fifth Elephant workshop\n",
    "\n",
    "Welcome to the LLM fine-tuning workshop and thanks for your interest. In this session, we will walk you through the various steps in fine-tuning your own Large Language Model (LLM). This is a continuous area of research and evolution so the information contained here can change frequently. We have also made decisions for this workshop with the aim of simplifying the process and hope that it provides a great starting point from which participants can explore further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build an AI script writer whom we can include in the writer's room\n",
    "\n",
    "The Writer's room is a fabled place where several writers that are working on a show come together with the Showrunner, Executive producer and others to write the script of a particular show. Each writer's room works and operates differently but they all include brainstorming for ideas, identifying characters, specific plot elements and in the end produce a detailed script for an episode for a series/show/play. The idea itself is not new and there has been research in this area even before the recent popularity of LLMs. In the LLM-era there have been two interesting ideas that stand out - [Showrunner Agents](https://fablestudio.github.io/showrunner-agents/) - where researchers built out an entire episode of South Park by combinging various types of LLM-based agents to work together. The other approach is a paper called [Dramatron (from Deepmind)](https://arxiv.org/pdf/2209.14958) which used a series of prompts that were used to create plays that were finally staged as well.\n",
    "\n",
    "In this workshop, we will take inspiration from the Dramatron paper to create our own scriptwriting assistant by fine-tuning our own LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the slides, here is a simple step-by-step approach to fine-tuning our own LLM.\n",
    "\n",
    "<a href=\"https://ibb.co/bQyLGNx\"><img src=\"https://i.ibb.co/mbYtPhm/Screenshot-2024-07-04-at-14-46-19.png\" alt=\"Screenshot-2024-07-04-at-14-46-19\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our training config\n",
    "\n",
    "We choose to use the Axolotl package for running our fine-tuning. It is a wrapper on top of various other packages and provides a simple but detailed configuration file where all parameters for a specific fine-tuning job are specified. There are also several example configurations available from which we can start and adapt as required - this provides a great onboarding experience.\n",
    "\n",
    "Let's take a look at what our configuration file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "### Configuration file for a training job that teaches Mistral 7B v0.1 to memorize a small batch from the SQLQA dataset\n",
    "#######\n",
    "\n",
    "###\n",
    "# Model Configuration: Mistral 7B\n",
    "###\n",
    "base_model: mistralai/Mistral-7B-v0.1\n",
    "\n",
    "# base model weight quantization\n",
    "load_in_8bit: false\n",
    "load_in_4bit: false\n",
    "\n",
    "# attention implementation\n",
    "flash_attention: true\n",
    "\n",
    "# finetuned adapter config\n",
    "adapter: lora\n",
    "lora_model_dir:\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.0 # off because this is a memorization test\n",
    "lora_target_linear: true\n",
    "lora_modules_to_save: # required when adding new tokens to LLaMA/Mistral\n",
    "  - embed_tokens\n",
    "  - lm_head\n",
    "# for details, see https://github.com/huggingface/peft/issues/334#issuecomment-1561727994\n",
    "\n",
    "###\n",
    "# Dataset Configuration: sqlqa\n",
    "###\n",
    "\n",
    "datasets:\n",
    "  # This will be the path used for the data when it is saved to the Volume in the cloud.\n",
    "  - path: data.jsonl\n",
    "    ds_type: json\n",
    "    type:\n",
    "      # JSONL file contains question, context, answer fields per line.\n",
    "      # This gets mapped to instruction, input, output axolotl tags.\n",
    "      field_instruction: question\n",
    "      field_input: context\n",
    "      field_output: answer\n",
    "      # Format is used by axolotl to generate the prompt.\n",
    "      format: |-\n",
    "        [INST] Using the schema context below, generate a SQL query that answers the question.\n",
    "        {input}\n",
    "        {instruction} [/INST]\n",
    "\n",
    "# dataset formatting config\n",
    "tokens: # add new control tokens from the dataset to the model\n",
    "  - \"[INST]\"\n",
    "  - \" [/INST]\"\n",
    "  - \"[SQL]\"\n",
    "  - \" [/SQL]\"\n",
    "\n",
    "special_tokens:\n",
    "  bos_token: \"<s>\"\n",
    "  eos_token: \"</s>\"\n",
    "  unk_token: \"<unk>\"\n",
    "\n",
    "train_on_inputs: false\n",
    "\n",
    "val_set_size: 0.5\n",
    "\n",
    "# dataset packing config\n",
    "sequence_len: 4096\n",
    "sample_packing: false\n",
    "eval_sample_packing: false\n",
    "pad_to_sequence_len: false\n",
    "group_by_length: false\n",
    "\n",
    "###\n",
    "# Training Configuration: AdamW, CosineLR, deepspeed, many epochs\n",
    "###\n",
    "\n",
    "# random seed for better reproducibility\n",
    "seed: 117\n",
    "\n",
    "# optimizer config\n",
    "optimizer: adamw_torch\n",
    "learning_rate: 0.0001\n",
    "lr_scheduler: cosine\n",
    "warmup_steps: 10\n",
    "gradient_accumulation_steps: 1\n",
    "micro_batch_size: 16\n",
    "weight_decay: 0.0\n",
    "\n",
    "# axolotl saving config\n",
    "dataset_prepared_path: last_run_prepared\n",
    "output_dir: ./lora-out\n",
    "\n",
    "# logging and eval config\n",
    "logging_steps: 10\n",
    "eval_steps: 10\n",
    "save_strategy: \"no\"\n",
    "num_epochs: 50\n",
    "\n",
    "# wandb logging config\n",
    "wandb_project: memorize-sqlqa\n",
    "\n",
    "# training performance optimization config\n",
    "bf16: auto\n",
    "fp16: false\n",
    "tf32: false\n",
    "deepspeed: /workspace/axolotl/deepspeed_configs/zero3_bf16.json\n",
    "gradient_checkpointing: true\n",
    "\n",
    "###\n",
    "# Miscellaneous Configuration\n",
    "###\n",
    "\n",
    "# prevents over-writing the config from the CLI\n",
    "strict: false\n",
    "\n",
    "# run with debug-level logs\n",
    "debug:\n",
    "\n",
    "# \"Don't mess with this, it's here for accelerate and torchrun\" -- axolotl docs\n",
    "local_rank:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Fine-tuning Dataset\n",
    "\n",
    "One of the most important aspects when fine-tuning an LLM is the dataset on which you would like to fine-tune. There are several datasets that are available on the HuggingFace datasets repository but this is also where your unique business advantage or proprietary data comes to play. For instance, let's say that you already run an app in production with multiple users adopting it, you would typically start by making use of GPT-3.5 or GPT-4 as the LLM. Over time, you likley run multiple LLM calls and store the historical information. Not all responses maybe accurate and you will also know what is actually the desired response. You would therefore use this data and annotate or label it manually. This could be a great asset for you to use and build a specific LLM that cannot be easily replaced and is also chepaer.\n",
    "\n",
    "For this workshop, since we are not dealing with an existing app as such we followed the route of creating a synthetic dataset with the help of OpenAI APIs. Basically, we take an existing example and ask the OpenAI LLM to create multiple other examples that follow the same style but add some diversity to the examples. You can also run this via a batch API that is cheaper - although you might need to wait upto 24 hours for it to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will dive into a different notebook called \"Synthetic_Data_Generation\" to look at the details on how one can generate a dataset. Feel free to use the techniques and prompts provided for your own use case. However, to speed up the process of the workshop I have also uploaded the dataset that I created to HuggingFace datasets and it is also available in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Fine-tuning process\n",
    "\n",
    "We have our dataset, we have decided the configuration and now it's time to run our fine-tuning process. Till now we were dealing with conceptual topics and now we get to the nitty-gritty of making this whole thing work. This is typically also the point where you will run into the most number of issues. One of the biggest bottlenecks with fine-tuning LLMs is the requirement to have GPUs. The smallest models (with 7 billion parameters) are still quite large and not everyone has a GPU lying around. This is where it makes sense to leverage a cloud provider.\n",
    "In addition to that, many of the fine-tuning libraries are still work in progress with a lot of rough edges. So it's not uncommon to run into issues like installation dependencies, incorrect deployments when moving to the cloud and much else. \n",
    "\n",
    "Keeping the goal of this workshop in mind, we chose to make use of Modal Labs as the GPU provider and make use of a starter script that they have already created to get our fine-tuning job to run. This script takes care of multiple aspects of running a GPU training process like creating the necessarsy docker containers, kicking off a distributed training run, merging the LoRA at the end and finally also spinning up an inference server. We will obviously incur costs in this process but Modal also provides upto 30 USD of free credit with every account each month which should be enough for running many of our fine-tuning processes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modal run --detach src.train --config=config/ai_script_writer.yml --data=data/fine_tuning.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference\n",
    "\n",
    "Now that we have trained our model, let us now try and see how it works - moment of truth!\n",
    "\n",
    "For running inference also there are several options. Modal allows you to have a serverless inference instance that can be spun up only for serving a single request and then it's shut down again. This way, you are not incurring GPU costs when there are no requests. Of course, the issue is that you will also have to run into start-up wait times. In the default configuration, we have set a time out of 15 minutes. So if no new requests are served for 15 minutes then the server would be shut down automatically.\n",
    "\n",
    "In the following section, we will create a simple Gradio app to provide the input and read the output responses from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`modal run -q src.inference --prompt \"[INST] Logline: A story about a famed German baker who must decide what to do with his beloved and popular bakery as he thinks about the future\n",
    "[/INST]`\n",
    "\n",
    "`modal deploy src.inference`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /usr/local/python/3.10.13/lib/python3.10/site-packages (4.37.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (5.3.0)\n",
      "Requirement already satisfied: fastapi in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (0.111.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==1.0.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (1.0.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/codespace/.local/lib/python3.10/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (0.23.4)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/codespace/.local/lib/python3.10/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/codespace/.local/lib/python3.10/site-packages (from gradio) (3.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /home/codespace/.local/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (3.10.6)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from gradio) (24.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/codespace/.local/lib/python3.10/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/codespace/.local/lib/python3.10/site-packages (from gradio) (10.3.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (2.8.0)\n",
      "Requirement already satisfied: pydub in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/codespace/.local/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (0.5.1)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/codespace/.local/lib/python3.10/site-packages (from gradio) (4.12.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (2.0.7)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio) (0.30.1)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from gradio-client==1.0.2->gradio) (2024.5.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gradio-client==1.0.2->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/codespace/.local/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.22.0)\n",
      "Requirement already satisfied: toolz in /usr/local/python/3.10.13/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (4.4.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.52.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.20.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from fastapi->gradio) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from fastapi->gradio) (0.0.4)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from fastapi->gradio) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from fastapi->gradio) (2.2.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def http_bot(prompt):\n",
    "    url = 'https://sidhusmart--example-axolotl-inference-web.modal.run'\n",
    "    params = {\n",
    "        'input': prompt\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://45b2e6a5a5cfe239ab.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://45b2e6a5a5cfe239ab.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Fine-tuned LLM Completion\\n\")\n",
    "        inputbox = gr.Textbox(label=\"Input\", placeholder=\"Enter text and press ENTER\")\n",
    "        outputbox = gr.Textbox(label=\"Output\", placeholder=\"Generated result from the model\")\n",
    "        inputbox.submit(http_bot, [inputbox], [outputbox])\n",
    "\n",
    "demo.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
